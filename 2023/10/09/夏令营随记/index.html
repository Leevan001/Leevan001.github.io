<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>夏令营随记 | 早早起床，拥抱太阳</title><meta name="author" content="如风"><meta name="copyright" content="如风"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="相见恨晚的网站： https:&#x2F;&#x2F;noobdream.com&#x2F;Practice&#x2F;index&#x2F; https:&#x2F;&#x2F;www.zhihu.com&#x2F;column&#x2F;c_1425969515960590336  AIlab面试题目准备 如何防止过拟合 为了得到一致假设而使假设变得过度复杂称为过拟合(overfitting)，过拟合表现在训练好的模型在训练集上效果很好，但是在测试集上效果差。  模型的泛化能力弱">
<meta property="og:type" content="article">
<meta property="og:title" content="夏令营随记">
<meta property="og:url" content="https://blog.liyifan001.top/2023/10/09/%E5%A4%8F%E4%BB%A4%E8%90%A5%E9%9A%8F%E8%AE%B0/index.html">
<meta property="og:site_name" content="早早起床，拥抱太阳">
<meta property="og:description" content="相见恨晚的网站： https:&#x2F;&#x2F;noobdream.com&#x2F;Practice&#x2F;index&#x2F; https:&#x2F;&#x2F;www.zhihu.com&#x2F;column&#x2F;c_1425969515960590336  AIlab面试题目准备 如何防止过拟合 为了得到一致假设而使假设变得过度复杂称为过拟合(overfitting)，过拟合表现在训练好的模型在训练集上效果很好，但是在测试集上效果差。  模型的泛化能力弱">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/166832600130917317FFACC42E4B5729AE3767B0311D0.png">
<meta property="article:published_time" content="2023-10-09T11:39:32.000Z">
<meta property="article:modified_time" content="2023-10-09T11:41:08.611Z">
<meta property="article:author" content="如风">
<meta property="article:tag" content="推免夏令营">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/166832600130917317FFACC42E4B5729AE3767B0311D0.png"><link rel="shortcut icon" href="/images/favicon.ico"><link rel="canonical" href="https://blog.liyifan001.top/2023/10/09/%E5%A4%8F%E4%BB%A4%E8%90%A5%E9%9A%8F%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 如风","link":"链接: ","source":"来源: 早早起床，拥抱太阳","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '夏令营随记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-10-09 19:41:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/1668273103133D8802D069515E0A42BC91FDE752B8A5C.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">34</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/166832600130917317FFACC42E4B5729AE3767B0311D0.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">早早起床，拥抱太阳</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">夏令营随记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-09T11:39:32.000Z" title="发表于 2023-10-09 19:39:32">2023-10-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-10-09T11:41:08.611Z" title="更新于 2023-10-09 19:41:08">2023-10-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8E%A8%E5%85%8D%E5%A4%8F%E4%BB%A4%E8%90%A5/">推免夏令营</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>31分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="夏令营随记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>相见恨晚的网站：</p>
<p><a target="_blank" rel="noopener" href="https://noobdream.com/Practice/index/">https://noobdream.com/Practice/index/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/c_1425969515960590336">https://www.zhihu.com/column/c_1425969515960590336</a></p>
<img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16878314229984fdbbf9b628be9a45fe096fba16e261.jpg" alt="16878314229984fdbbf9b628be9a45fe096fba16e261.jpg" style="zoom:50%;" />
<h1>AIlab面试题目准备</h1>
<h2 id="如何防止过拟合">如何防止过拟合</h2>
<p>为了得到一致假设而使假设变得过度复杂称为过拟合(overfitting)，<strong>过拟合表现在训练好的模型在训练集上效果很好，但是在测试集上效果差</strong>。</p>
<blockquote>
<p>模型的泛化能力弱</p>
</blockquote>
<p>解决方法：</p>
<ol>
<li>数据集扩增（Data Augmentation）
<ol>
<li>在物体分类（object recognition）问题中，数据扩增已经成为一项特殊的有效的技术。物体在图像中的位置、姿态、尺度，整体图片敏感度等都不会影响分类结果，所以我们就可以通过图像平移、翻转、缩放、切割等手段将数据库成倍扩充。</li>
<li>语音识别（speech recognition）中，加入噪音也被看做是一种数据扩增方式。</li>
</ol>
</li>
<li>改进模型
<ol>
<li><strong>过拟合主要有两个原因造成的，数据太少和模型太复杂</strong></li>
<li>Early Stopping的做法就是运行优化方法直到若干次在验证集上的验证误差没有提升时候停止。</li>
<li><strong>正则化（regularization）</strong>
<ol>
<li>损失函数分为经验风险损失函数和结构风险损失函数，结构风险损失函数就是经验损失函数+表示模型复杂度的正则化</li>
</ol>
</li>
<li>Dropout
<ol>
<li>Dropout方法通过修改隐藏层神经元的个数来防止网络的过拟合，也就是通过修改深度网络本身。</li>
<li>Dropout按照给定的概率P随机剔除一些神经元，只有没有被剔除也就是被保留下来的神经元的参数被更新。</li>
<li><strong>每一批次数据，由于随机性剔除神经元，使得网络具有一定的稀疏性，从而能减轻了不同特征之间的协同效应。而且由于每次被剔除的神经元不同，所以整个网络神经元的参数也只是部分被更新，消除减弱了神经元间的联合适应性，增强了神经网络的泛化能力和鲁棒性。Dropout只在训练时使用，作为一个超参数，然而在测试集时，并不能使用。</strong></li>
</ol>
</li>
<li>深度学习中两种多任务学习模式：隐层参数的硬共享和软共享
<ol>
<li>硬共享机制是指在所有任务中共享隐藏层，同时保留几个特定任务的输出层来实现。硬共享机制降低了过拟合的风险。多个任务同时学习，模型就越能捕捉到多个任务的同一表示，从而导致模型在原始任务上的过拟合风险越小。</li>
<li>软共享机制是指每个任务有自己的模型，自己的参数。模型参数之间的距离是正则化的，以便保障参数相似性。</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="gan的思想">GAN的思想</h2>
<h3 id="对抗样本">对抗样本</h3>
<p><strong>对抗样本(adversarial example)，它是指经过精心计算得到的用于误导分类器的样本</strong>。例如下图就是一个例子，左边是一个熊猫，但是添加了少量随机噪声变成右图后，分类器给出的预测类别却是长臂猿，但视觉上左右两幅图片并没有太大改变。</p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16878325449991687832544397.png" alt="16878325449991687832544397.png"></p>
<p><strong>图像分类器本质上是高维空间的一个复杂的决策边界</strong>。</p>
<p>训练完成后，分类器是无法泛化到所有数据上，除非我们的训练集包含了分类类别的所有数据，但实际上我们做不到。而做不到泛化到所有数据的分类器，其实就会过拟合训练集的数据，这也就是我们可以利用的一点。</p>
<p>在 L2 范数看来，对于熊猫和长臂猿的决策边界并没有那么远，添加了非常微弱的随机噪声的图片可能就远离了熊猫的决策边界内，到达长臂猿的预测范围内，因此欺骗了分类器。</p>
<p>GAN 的基本思想就是一个最小最大定理，当两个玩家（D 和 G）彼此竞争时（零和博弈），双方都假设对方采取最优的步骤而自己也以最优的策略应对（最小最大策略），那么结果就已经预先确定了，玩家无法改变它（纳什均衡）。</p>
<h2 id="batch-normalization介绍一下-它在训练和测试是有什么区别">Batch normalization介绍一下? 它在训练和测试是有什么区别?</h2>
<p>Conv=&gt;BN=&gt;ReLU=&gt;dropout=&gt;Conv</p>
<p>BN本质上是解决传播过程中的<strong>梯度消失</strong>问题</p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16878329940001687832993487.png" alt="16878329940001687832993487.png"></p>
<ul>
<li>这个可学习重构参数 γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布。（因为如果没有 γ、β ，那相当于我这一层网络所学习到的特征分布被你搞坏了）</li>
<li>这个 scale 和 shift ，它们的主要作用就是找到一个线性和非线性的平衡点，既能享受非线性较强的表达能力，有可以避免非线性饱和导致网络收敛变慢问题</li>
</ul>
<p>BN层是对于每个神经元做归一化处理，甚至只需要对某一个神经元进行归一化，而不是对一整层网络的神经元进行归一化。既然BN是对单个神经元的运算，那么在CNN中卷积层上要怎么搞？假如某一层卷积层有6个特征图，每个特征图的大小是100100，这样就相当于这一层网络有6100100个神经元，如果采用BN，就会有6100*100个参数γ、β，这样岂不是太恐怖了。因此卷积层上的BN使用，其实也是使用了类似权值共享的策略，把一整张特征图当做一个神经元进行处理。</p>
<p>卷积神经网络经过卷积后得到的是一系列的特征图，如果min-batch sizes为m，那么网络某一层输入数据可以表示为四维矩阵 (m,f,w,h)，m为min-batch sizes，f为特征图个数，w、h分别为特征图的宽高。在CNN中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch Normalization，mini-batch size 的大小就是：m ∗ w ∗ h m * w * hm∗w∗h，于是对于每个特征图都只有一对可学习参数：γ、β。说白了吧，这就是相当于求取所有样本所对应的一个特征图的所有神经元的平均值、方差，然后对这个特征图神经元做归一化。</p>
<p>在使用BN前，减小学习率、小心的权重初始化的目的是：使其输出的数据分布不要发生太大的变化。</p>
<p>因为偏置参数 b 经过 BN 层后其实是没有用的，最后也会被均值归一化，当然BN层后面还有个 β 参数作为偏置项，所以 b 这个参数就可以不用了。</p>
<h2 id="vision-transformer">Vision Transformer?</h2>
<p>对任意输入尺寸图片，怎么样将其转换为 token序列</p>
<p>当输入图片尺寸发生变化时，由于每个 patch 的尺寸固定，图片切分出的 patch 数就会发生变化。表现在上述特征图中，就是特征图的尺寸发生了变化。这样一来，我们原本位置编码图的尺寸就和图像特征图的尺寸对不上了，无法进行后续的计算。</p>
<p>找到了问题所在，解决的方法也就顺理成章了。位置编码代表的是 patch 所在位置的附加信息，那么如果和图像特征图的尺寸不匹配，只需要使用双三次插值法（Bicubic）对位置编码图进行插值缩放，缩放到与图像特征图一致的尺寸，就同样可以表现每个 patch 在图片中的位置信息。</p>
<h2 id="目标检测中输入检测器之前要做什么操作-attention的作用是什么">目标检测中输入检测器之前要做什么操作? Attention的作用是什么</h2>
<p>在目标检测中，输入检测器之前通常需要进行一系列的<font color='red'>预处理</font>操作。以下是常见的预处理步骤：</p>
<ol>
<li><font color='orange'>图像加载</font>：将待检测的图像加载到内存中，以便进行后续处理。</li>
<li><font color='orange'>图像归一化</font>：对图像进行归一化操作，使其具有统一的尺度和范围。常见的方法是将像素值缩放到0到1之间或者使用标准化方法（例如减去均值并除以标准差）。</li>
<li><font color='orange'>图像调整</font>：对图像进行调整以适应检测器的输入要求。这可能包括调整图像的大小、剪裁或填充图像等操作。</li>
<li><font color='orange'>数据增强</font>：为了增加数据的多样性和模型的鲁棒性，可以应用一些数据增强技术，如随机旋转、翻转、缩放或裁剪等。</li>
</ol>
<p>以上这些预处理步骤的目的是为了将输入图像转换为检测器所需的形式，以提高检测器的性能和准确性。</p>
<p>关于Attention的作用，它是一种机制，用于在深度学习模型中引入对不同部分的关注或重要性加权。在目标检测中，Attention机制可以用于引导模型更加关注与目标相关的区域或特征，从而提高检测的准确性和鲁棒性。</p>
<p>在目标检测任务中，Attention机制可以应用于多个方面，例如：</p>
<ol>
<li><font color='orange'>区域选择</font>：通过对感兴趣区域（Region of Interest）进行Attention加权，使模型能够更关注与目标相关的区域，减少对背景区域的关注。</li>
<li><font color='orange'>特征融合</font>：在多个特征图之间应用Attention机制，以更好地结合不同尺度和层次的特征信息，从而提高模型的表达能力。</li>
<li><font color='orange'>尺度适应</font>：对于多尺度目标检测，可以使用Attention机制来调整模型对不同尺度目标的关注程度，使其能够适应不同大小的目标。</li>
</ol>
<p>通过引入Attention机制，模型可以更加自适应地关注重要的目标信息，提高目标检测的性能和效果。</p>
<p>视觉注意力一般分为三种，即<strong>通道域、空间域和混合域</strong>（前两者都使用）。</p>
<p><strong>通道域</strong><br>
通道域注意力的代表作是SENet，其实现思想和过程都非常简单，仅仅加入了一个即插即用的SE模块分支用于建模不同channel之间的相关性就大幅度地提升了模型性能（最后一届ImageNet图像识别的冠军，超过去年冠军25%）。</p>
<h2 id="lstm">LSTM</h2>
<p>LSTM从被设计之初就被用于解决一般递归神经网络中普遍存在的<strong>长期依赖问题</strong>，使用LSTM可以有效的传递和表达长时间序列中的信息并且不会导致长时间前的有用信息被忽略（遗忘）。与此同时，LSTM还可以解决RNN中的梯度消失/爆炸问题。</p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16880886061211688088605699.png" alt="16880886061211688088605699.png"></p>
<p>LSTM四个函数层</p>
<p>遗忘门</p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16880885581921688088558013.png" alt="16880885581921688088558013.png"></p>
<p>记忆门</p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16880887300541688088728146.png" alt="16880887300541688088728146.png"></p>
<p>输出门</p>
<h2 id="transformer">Transformer</h2>
<img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16878592399891687859238260.png" alt="16878592399891687859238260.png" style="zoom:33%;" />
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16878592669931687859265194.png" alt="16878592669931687859265194.png"></p>
<p>Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。</p>
<img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16878593379951687859337852.png" alt="16878593379951687859337852.png" style="zoom:50%;" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>): </span><br><span class="line"><span class="comment"># query, key, value的形状类似于(30, 8, 10, 64), (30, 8, 11, 64), </span></span><br><span class="line"><span class="comment">#(30, 8, 11, 64)，例如30是batch.size，即当前batch中有多少一个序列；</span></span><br><span class="line"><span class="comment"># 8=head.num，注意力头的个数；</span></span><br><span class="line"><span class="comment"># 10=目标序列中词的个数，64是每个词对应的向量表示；</span></span><br><span class="line"><span class="comment"># 11=源语言序列传过来的memory中，当前序列的词的个数，</span></span><br><span class="line"><span class="comment"># 64是每个词对应的向量表示。</span></span><br><span class="line"><span class="comment"># 类似于，这里假定query来自target language sequence；</span></span><br><span class="line"><span class="comment"># key和value都来自source language sequence.</span></span><br><span class="line">  <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span> </span><br><span class="line">  d_k = query.size(-<span class="number">1</span>) <span class="comment"># 64=d_k</span></span><br><span class="line">  scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / </span><br><span class="line">    math.sqrt(d_k) <span class="comment"># 先是(30,8,10,64)和(30, 8, 64, 11)相乘，</span></span><br><span class="line">    <span class="comment">#（注意是最后两个维度相乘）得到(30,8,10,11)，</span></span><br><span class="line">    <span class="comment">#代表10个目标语言序列中每个词和11个源语言序列的分别的“亲密度”。</span></span><br><span class="line">    <span class="comment">#然后除以sqrt(d_k)=8，防止过大的亲密度。</span></span><br><span class="line">    <span class="comment">#这里的scores的shape是(30, 8, 10, 11)</span></span><br><span class="line">  <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">    scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>) </span><br><span class="line">    <span class="comment">#使用mask，对已经计算好的scores，按照mask矩阵，填-1e9，</span></span><br><span class="line">    <span class="comment">#然后在下一步计算softmax的时候，被设置成-1e9的数对应的值~0,被忽视</span></span><br><span class="line">  p_attn = F.softmax(scores, dim = -<span class="number">1</span>) </span><br><span class="line">    <span class="comment">#对scores的最后一个维度执行softmax，得到的还是一个tensor, </span></span><br><span class="line">    <span class="comment">#(30, 8, 10, 11)</span></span><br><span class="line">  <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">    p_attn = dropout(p_attn) <span class="comment">#执行一次dropout</span></span><br><span class="line">  <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"><span class="comment">#返回的第一项，是(30,8,10, 11)乘以（最后两个维度相乘）</span></span><br><span class="line"><span class="comment">#value=(30,8,11,64)，得到的tensor是(30,8,10,64)，</span></span><br><span class="line"><span class="comment">#和query的最初的形状一样。另外，返回p_attn，形状为(30,8,10,11). </span></span><br><span class="line"><span class="comment">#注意，这里返回p_attn主要是用来可视化显示多头注意力机制。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module): </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>): </span><br><span class="line">    <span class="comment"># h=8, d_model=512</span></span><br><span class="line">    <span class="string">&quot;Take in model size and number of heads.&quot;</span> </span><br><span class="line">    <span class="built_in">super</span>(MultiHeadedAttention, self).__init__() </span><br><span class="line">    <span class="keyword">assert</span> d_model % h == <span class="number">0</span> <span class="comment"># We assume d_v always equals d_k 512%8=0</span></span><br><span class="line">    self.d_k = d_model // h <span class="comment"># d_k=512//8=64</span></span><br><span class="line">    self.h = h <span class="comment">#8</span></span><br><span class="line">    self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>) </span><br><span class="line">    <span class="comment">#定义四个Linear networks, 每个的大小是(512, 512)的，</span></span><br><span class="line">    <span class="comment">#每个Linear network里面有两类可训练参数，Weights，</span></span><br><span class="line">    <span class="comment">#其大小为512*512，以及biases，其大小为512=d_model。</span></span><br><span class="line"></span><br><span class="line">    self.attn = <span class="literal">None</span> </span><br><span class="line">    self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>): </span><br><span class="line">   <span class="comment"># 注意，输入query的形状类似于(30, 10, 512)，</span></span><br><span class="line">   <span class="comment"># key.size() ~ (30, 11, 512), </span></span><br><span class="line">   <span class="comment">#以及value.size() ~ (30, 11, 512)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># Same mask applied to all h heads. </span></span><br><span class="line">      mask = mask.unsqueeze(<span class="number">1</span>) <span class="comment"># mask下回细细分解。</span></span><br><span class="line">    nbatches = query.size(<span class="number">0</span>) <span class="comment">#e.g., nbatches=30</span></span><br><span class="line">    <span class="comment"># 1) Do all the linear projections in batch from </span></span><br><span class="line">    <span class="comment">#d_model =&gt; h x d_k </span></span><br><span class="line">    query, key, value = [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k)</span><br><span class="line">      .transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> l, x <span class="keyword">in</span> </span><br><span class="line">      <span class="built_in">zip</span>(self.linears, (query, key, value))] </span><br><span class="line">      <span class="comment"># 这里是前三个Linear Networks的具体应用，</span></span><br><span class="line">      <span class="comment">#例如query=(30,10, 512) -&gt; Linear network -&gt; (30, 10, 512) </span></span><br><span class="line">      <span class="comment">#-&gt; view -&gt; (30,10, 8, 64) -&gt; transpose(1,2) -&gt; (30, 8, 10, 64)</span></span><br><span class="line">      <span class="comment">#，其他的key和value也是类似地，</span></span><br><span class="line">      <span class="comment">#从(30, 11, 512) -&gt; (30, 8, 11, 64)。</span></span><br><span class="line">    <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">    x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">      dropout=self.dropout) </span><br><span class="line">      <span class="comment">#调用上面定义好的attention函数，输出的x形状为(30, 8, 10, 64)；</span></span><br><span class="line">      <span class="comment">#attn的形状为(30, 8, 10=target.seq.len, 11=src.seq.len)</span></span><br><span class="line">    <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">    x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().</span><br><span class="line">      view(nbatches, -<span class="number">1</span>, self.h * self.d_k) </span><br><span class="line">      <span class="comment"># x ~ (30, 8, 10, 64) -&gt; transpose(1,2) -&gt; </span></span><br><span class="line">      <span class="comment">#(30, 10, 8, 64) -&gt; contiguous() and view -&gt; </span></span><br><span class="line">      <span class="comment">#(30, 10, 8*64) = (30, 10, 512)</span></span><br><span class="line"><span class="keyword">return</span> self.linears[-<span class="number">1</span>](x) </span><br><span class="line"><span class="comment">#执行第四个Linear network，把(30, 10, 512)经过一次linear network，</span></span><br><span class="line"><span class="comment">#得到(30, 10, 512).</span></span><br></pre></td></tr></table></figure>
<h1>数据结构</h1>
<p>抽象数据类型的三个组成部分分别为：</p>
<p>数据对象、数据关系和基本操作</p>
<p>四类基本结构</p>
<blockquote>
<p>1.集合</p>
<p>集合中的数据元素除了属于同一个类型外，没有其他关系</p>
<p>2.线性结构</p>
<p>线性结构中元素之间存在一对一关系.</p>
<p>3.树形结构</p>
<p>树形结构中元素之间存在一对多关系</p>
<p>4.图状结构(网状结构)</p>
<p>网状结构中元素之间存在多对多关系</p>
</blockquote>
<p><strong>数据的存储结构可用四种基本的存储方法表示，它们分别是顺序、链式、索引、散列。</strong></p>
<p>数据结构和数据类型两个概念之间有区别吗？</p>
<p>答：简单地说，数据结构定义了一组按某些关系结合在一起的数组元素。数据类型不仅定义了一组带结构的数据元素，而且还在其上定义了一组操作。</p>
<p><strong>简述线性结构与非线性结构的不同点。</strong></p>
<p><strong>答：线性结构反映结点间的逻辑关系是一对一的，非线性结构反映结点间的逻辑关系是多对多的。</strong></p>
<p>拓扑排序 时间复杂度 ：<strong>O (n+m)</strong> 因每个结点只需加入队列一次，而每个结点加入队列前需要进行入度数量次的操作，因此时间复杂度为 O (n+m)。</p>
<h1>计算机网络</h1>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16881113940181688111392545.png" alt="16881113940181688111392545.png"></p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16881114690891688111468586.png" alt="16881114690891688111468586.png"></p>
<p>链接：<a target="_blank" rel="noopener" href="https://www.nowcoder.com/questionTerminal/9c9ccfd543c74dbb9f46cb14a27dd273">https://www.nowcoder.com/questionTerminal/9c9ccfd543c74dbb9f46cb14a27dd273</a><br>
来源：牛客网</p>
<p>物理层：实现网络互连的主要设备有<strong>中继器和HUB(集线器)</strong>。中继器的主要功能是对接收到的信号进行再生整形放大以扩大网络的传输距离；集线器在此基础上将所有的节点集中在以它为中心的节点中，可组成星型拓扑结构。</p>
<p>数据链路层：实现网络互联的主要设备有<strong>二层交换机和网桥</strong>。交换机是一种基于MAC识别，能完成封装转发数据包功能的网络设备。它可以“学习”MAC地址，并把其存放在内部地址表中，当一个数据帧的目的地址在MAC地址表中有映射时，它被转发到连接目的节点的端口而不是所有端口。 交换机将局域网分为多个冲突域，每个冲突域都是有独立的宽带，因此大大提高了局域网的带宽。网桥是数据链路层互联的设备，在网络互联中可起到数据接收、地址过滤与数据转发的作用，可用来实现多个不同网络系统之间的数据交换。</p>
<p>网络层：实现网络互连的主要设备有<strong>三层交换机和路由器</strong>。路由器用于连接多个逻辑上分开的网络，具有判断网络地址和选择IP路径的功能，它能在多网络互联环境中，建立灵活的连接，可用完全不同的数据分组和介质访问方法连接各种子网。</p>
<p>传输层（包括传输层）以上：实现网络互连的设备有<strong>网关</strong>。网关在网络层以上实现网络互连，用于两个高层协议不同的网络互连。与网桥只是简单地传达信息不同，网关对收到的信息要重新打包，以适应目的系统的需求。</p>
<p>以下属于物理层的设备是（）</p>
<p><strong>A. 中继器</strong>         B. 以太网交换机</p>
<p>C. 桥           D. 网关</p>
<p>ARP协议    Address Resolution Protocol   地址解析协议</p>
<p>在局域网中，网络中实际传输的是“帧”，帧里面是有目标主机的MAC地址的。在以太网中，一个主机要和另一个主机进行直接通信，必须</p>
<p>要知道目标主机的MAC地址。但这个目标MAC地址是如何获得的呢？它就是通过地址解析协议获得的。所谓“地址解析”就是主机在发送帧</p>
<p>之前将目标主机的IP地址转换成目标主机的MAC地址的过程。ARP协议的基本功能就是通过目标主机的IP地址，查询目标主机的MAC地址，以保证通信的顺利进行。协议又称做服务，ARP协议也即ARP服务，提供把IP地址转换成MAC地址的服务！</p>
<p>ARP 协议的作用是 （）</p>
<p>A、将端口号映射到IP 地址</p>
<p>B、连接IP 层和TCP 层</p>
<p>C、广播IP 地址</p>
<p><strong>D、将IP 地址映射到第二层地址</strong></p>
<p>A、将端口号映射到IP地址：这通常是通过网络地址转换（Network Address Translation，<strong>NAT</strong>）来实现的。NAT是一种在网络设备（例如路由器）上执行的技术，它允许多个主机共享单个公共IP地址。当数据包从私有网络中的主机发送到公共网络时，源IP地址和端口号会被修改为路由器的公共IP地址和一个新的端口号。这样，通过映射端口号，可以将传入的数据包正确路由到相应的主机。</p>
<p>B、连接IP层和TCP层：IP（Internet Protocol）层是在网络通信中负责将数据包从源主机传输到目标主机的层。而TCP（Transmission Control Protocol）层是运行在IP层之上的一种传输层协议，负责提供可靠的、面向连接的数据传输。IP层和TCP层之间的连接通常是通过套接字（socket）接口实现的。套接字提供了一组API，使应用程序能够创建TCP连接、发送数据和接收数据。</p>
<p>C、广播IP地址：IP地址广播是将数据包发送到网络中的所有主机的一种机制。在IPv4网络中，广播地址通常被定义为目标IP地址的网络部分全为1、主机部分全为0的地址。例如，如果一个网络的IP地址是192.168.0.0/24，那么该网络的广播地址就是192.168.0.255。发送到广播地址的数据包将被路由器转发到该网络中的所有主机。</p>
<p>D、将IP地址映射到第二层地址：第二层地址通常是指MAC（Media Access Control）地址，它是用于在局域网中唯一标识网络接口的地址。IP地址到MAC地址的映射是通过<strong>ARP</strong>（Address Resolution Protocol）来实现的。当主机需要发送数据包到目标IP地址时，它首先会查询本地的ARP缓存表，如果找不到对应的MAC地址，就会发送一个ARP请求广播，询问局域网上的其他主机谁拥有目标IP地址对应的MAC地址。目标主机收到ARP请求后，会回复一个包含自己MAC地址的ARP响应，从而建立起IP地址到MAC地址的映射关系。</p>
<p>第一层：<font color='red'>物理层</font>（PhysicalLayer)，规定通信设备的机械的、电气的、功能的和规程的特性，用以建立、维护和拆除物理链路连接。具体地讲，机械特性规定了网络连接时所需接插件的规格尺寸、引脚数量和排列情况等；电气特性规定了在物理连接上传输bit流时线路上信号电平的大小、阻抗匹配、传输速率距离限制等；功能特性是指对各个信号先分配确切的信号含义，即定义了DTE和DCE之间各个线路的功能；规程特性定义了利用信号线进行bit流传输的一组操作规程，是指在物理连接的建立、维护、交换信息时，DTE和DCE双方在各电路上的动作系列。<br>
在这一层，数据的单位称为<font color='red'>比特</font>（bit）。<br>
属于物理层定义的典型规范代表包括：EIA/TIA RS-232、EIA/TIA RS-449、V.35、RJ-45等。<br>
第二层：<font color='red'>数据链路层</font>（DataLinkLayer):在物理层提供比特流服务的基础上，建立相邻结点之间的数据链路，通过差错控制提供数据帧（Frame）在信道上无差错的传输，并进行各电路上的动作系列。在这一层，数据的单位称为<font color='red'>帧</font>（frame）。数据链路层协议的代表包括：SDLC、HDLC、PPP、STP、帧中继等。<br>
第三层是<font color='red'>网络层</font>(Network layer)<br>
在计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点，确保数据及时传送。网络层将数据链路层提供的帧组成数据包，包中封装有网络层包头，其中含有逻辑地址信息- -源站点和目的站点地址的网络地址。在这一层，数据的单位称为数据包（packet）。网络层协议的代表包括：IP、IPX、RIP、OSPF等。<br>
第四层是处理信息的<font color='red'>传输层</font>(Transport layer)。第4层的数据单元也称作<font color='red'>数据包</font>（packets）。但是，当你谈论TCP等具体的协议时又有特殊的叫法，TCP的数据单元称为段（segments）而UDP协议的数据单元称为“数据报（datagrams）”。这个层负责获取全部信息，因此，它必须跟踪数据单元碎片、乱序到达的数据包和其它在传输过程中可能发生的危险。第4层为上层提供端到端（最终用户到最终用户）的透明的、可靠的数据传输服务。所谓透明的传输是指在通信过程中传输层对上层屏蔽了通信传输系统的具体细节。传输层协议的代表包括：TCP、UDP、SPX等。<br>
在会话层及以上的高层次中，数据传送的单位不再另外命名，统称为报文。OSI将层与层之间交换的数据的单位称为服务数据单元SDU。</p>
<h1>操作系统</h1>
<p>绝对装入方式<br>
官方解释：在编译时，如果知道程序驻留在内存的什么位置，那么编译程序将产生绝对地址的目标代码。装入模块装入内存后，程序中的逻辑地址与实际内存地址完全相同，不需对程序的数据和地址进行修改。程序种所使用的绝对地址，可在汇编或编译时给出，也可由程序员赋予，通常在程序中采用符号地址，然后再汇编或编译时，再将这些符号地址转换为绝对地址<br>
补充：一般适用于单道批处理系统<br>
个人理解：在编译时，程序由高级语言需要向低级语言转变，首先要由高级语言转换为汇编语言，然后再将汇编语言转换为机器语言装入内存，再装入内存的时候如果采用绝对装入方式定位的话，有两种情况，第一种是，程序的逻辑地址等于物理地址，在装入的时候直接在存储区域的用户区装入（内存分为两部分：系统区+用户区），第二种情况是逻辑地址是程序员用符号语言（通常采用符号语言）赋予的，在汇编时再将这些符号地址转换为绝对地址，个人理解有误，望大佬看见指正</p>
<p>可重定位装入方式<br>
可充定位方式又称为静态定位装入方式，是因为程序在转换为汇编语言装入内存的时候会表名这个程序语句所占的物理内存是多少，在装入内存的时候就会根据用户区的起始地址+程序语句所占内存的大小，此时的地址为程序语句的实际物理地址，这样的装入方式称为可重定位装入方式，可以实现并发，但是一旦装入就无法再改变在内存中的物理位置</p>
<p>动态运行时装入方式<br>
在把装入模块装入内存后，不立即将相对地址转换为绝对地址，而是设置一个重定位寄存器，寄存器中存储的是，下一个执行程序应该加上的内存编号，进而转换为绝对地址，这样就可以进行多个程序的顺序装入，然后在运行时再将逻辑地址转换为绝对地址，使得内存的利用效率更高<br>
————————————————<br>
版权声明：本文为CSDN博主「zyl~~~」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>
原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43919396/article/details/105604790">https://blog.csdn.net/qq_43919396/article/details/105604790</a></p>
<h1>AI</h1>
<p>所以梯度消失出现的原因经常是因为<strong>网络层次过深</strong>，以及<strong>激活函数选择不当</strong>，比如sigmoid函数。</p>
<p>梯度消失的表现：</p>
<p>模型无法从训练数据中获得更新，损失几乎保持不变。</p>
<p>梯度爆炸<br>
梯度爆炸出现的原因：<br>
同梯度消失的原因一样，求解损失函数对参数的偏导数时，在梯度的连续乘法中总是遇上很大的绝对值，部分参数的梯度因为乘了很多较大的数而变得非常大，导致模型无法收敛。<br>
所以梯度爆炸出现的原因也是网络层次过深，或者权值初始化值太大。</p>
<p>梯度爆炸的表现：<br>
（1）模型型不稳定，更新过程中的损失出现显著变化。<br>
（2）训练过程中，模型损失变成 NaN。<br>
梯度消失爆炸的解决方法：<br>
重新设置网络结构，减少网络层数,调整学习率（消失增大，爆炸减小）。<br>
预训练加微调<br>
此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。<br>
激活函数采用relu,leaky relu,elu等。<br>
batch normalization<br>
更换参数初始化方法（对于CNN，一般用xavier或者msra的初始化方法）<br>
调整深度神经网络的结构<br>
使用残差模块，DESNET模块或LSTM等结构（避免梯度消失）<br>
l1、l2正则化（避免梯度爆炸）<br>
减小学习率、减小batch size（避免梯度爆炸）<br>
梯度裁剪（避免梯度爆炸）<br>
对于RNN，加入gradient clipping，每当梯度达到一定的阈值，就把他们设置回一个小一些的数字；</p>
<p>loss突然变nan的原因？<br>
可能原因：<br>
1、training sample中出现了脏数据，或输入数据未进行归一化<br>
2、学习速率过大，梯度值过大，产生梯度爆炸；<br>
3、在某些涉及指数计算，可能最后算得值为INF（无穷）（比如不做其他处理的softmax中分子分母需要计算exp（x），值过大，最后可能为INF/INF，得到NaN，此时你要确认你使用的softmax中在计算exp（x）做了相关处理（比如减去最大值等等））；<br>
4、不当的损失函数（尤其是自定义的损失函数时）；<br>
5、在卷积层的卷积步伐大于卷积核大小的时候。</p>
<h1>数学</h1>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16881119470901688111946559.png" alt="16881119470901688111946559.png"></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44114447">https://zhuanlan.zhihu.com/p/44114447</a></p>
<p>秩的本质是矩阵中的这几个向量所确定的空间的维数</p>
<p>Ax=b</p>
<p>一、第一类视角<br>
Ax=b</p>
<p>如果b存在于A的列张成的空间中，则有解，否则无解；<br>
如果b存在于A的列张成的空间中，且A的列均是线性无关的（列满秩），那么存在唯一解；<br>
如果b存在于A的列张成的空间中，但A的列是线性相关的（非列满秩），那么存在多解；<br>
二、第二类视角<br>
Ax=b</p>
<p>A如果行满秩，说明A的秩等于行满秩的秩，也就是A的列数只能大于等于行数（多解或者唯一解）<br>
A的列数等于A的行数，那么A的列是线性无关的，可以张成整个空间，对于任意的b存在解且唯一；<br>
A的列数大于A的行数，那么A的列是线性相关的，虽然可以张成整个空间，对于任意的b存在多解；<br>
A如果列满秩，那么A的列是线性无关的，说明A的秩等于列满秩的秩，也就是A的行数只能大于等于列数（无解或者唯一解）<br>
A的行数等于A的列数，退化成满秩问题，对于任意的b存在解且唯一；<br>
A的行数大于A的列数，情况分为2种（不存在多解的情况）：<br>
如果b不存在于A的列张成的空间中，则无解；<br>
如果b存在于A的列张成的空间中，则有唯一解；<br>
A既不是行满秩也不是列满秩（无解或者多解）<br>
如果b不存在于A的列张成的空间中，则无解；<br>
如果b存在于A的列张成的空间中，则有解，且是多解；（这里不考虑其他部分均0，可以退化成低维度满秩的情况，即不考虑[1 0; 0 0]）</p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16881129201001688112919639.png" alt="16881129201001688112919639.png"></p>
<p>古典概型、几何概型<br>
古典概型——有限等可能（有限个可能事件，且每个事件都是等可能概率事件）<br>
几何概型——无限等可能</p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16882067634421688206763222.png" alt="16882067634421688206763222.png"></p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16882071659631688207164986.png" alt="16882071659631688207164986.png"></p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16882072868961688207285039.png" alt="16882072868961688207285039.png"></p>
<p>什么是先验概率？</p>
<p>事情未发生，只根据以往数据统计，分析事情发生的可能性，即先验概率。</p>
<p>指根据以往经验和分析。在实验或采样前就可以得到的概率。</p>
<p>先验概率是指根据以往经验和分析得到的概率，如全概率公式，它往往作为&quot;由因求果&quot;问题中的&quot;因&quot;出现。</p>
<p><font color='red'>后验概率</font></p>
<p>事情已发生，已有结果，求引起这事发生的因素的可能性，由果求因，即后验概率。</p>
<p>指某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率。</p>
<p>后验概率是指依据得到&quot;结果&quot;信息所计算出的最有可能是那种事件发生,如贝叶斯公式中的,是&quot;执果寻因&quot;问题中的&quot;因&quot;。</p>
<p>与先验概率的关系<br>
后验概率的计算，是以先验概率为前提条件的。如果只知道事情结果，而不知道先验概率（没有以往数据统计），是无法计算后验概率的。</p>
<p>后验概率的计算需要应用到贝叶斯公式。</p>
<p><strong>随机变量的均值</strong>(不同于样本均值)，大数定律指出如果样本足够的话，样本均值会无限接近数学期望。</p>
<p>数学期望(mean)（或<a href="https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E5%9D%87%E5%80%BC/5922988">均值</a>，亦简称期望）是<strong>试验中每次可能结果的<a href="https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E6%A6%82%E7%8E%87/828845">概率</a>乘以其结果的总和</strong>，是最基本的<a href="https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E6%95%B0%E5%AD%A6/107037">数学</a>特征之一。它反映随机变量平均取值的大小。</p>
<p>方差是在概率论和统计方差衡量随机变量或一组数据时离散程度的度量。概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。统计中的方差（样本方差）是每个样本值与全体样本值的平均数之差的平方值的平均数。在许多实际问题中，研究方差即偏离程度有着重要意义。</p>
<p><img src="https://raw.githubusercontent.com/Leevan001/pictureBed/main/img/image-20230701193641760.png" alt="image-20230701193641760"></p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16882114239621688211423619.png" alt="16882114239621688211423619.png"></p>
<p>相关系数或协方差为 0 的时候能否说明两个分布无关？为什么？<br>
只能说明不线性相关，不能说明无关。因为在数学期望存在的情况下，独立必不相关，不相关未必独立。</p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16882115288911688211527169.png" alt="16882115288911688211527169.png"></p>
<p><strong>不相关</strong>就是两者<strong>没有线性关系</strong>，但是不排除其它关系存在；<strong>独立</strong>就是<strong>互不相干没有关联</strong>。</p>
<p>大数定律<br>
随机变量的均值依概率收敛于自己的期望。</p>
<p>大数定律通俗一点来讲，就是样本数量很大的时候，样本均值和数学期望充分接近，也就是说当我们大量重复某一相同的实验的时候，其最后的实验结果可能会稳定在某一数值附近。就像抛硬币一样，当我们不断地抛，抛个上千次，甚至上万次，我们会发现，正面或者反面向上的次数都会接近一半，也就是这上万次的样本均值会越来越接近 50% 这个真实均值，随机事件的频率近似于它的概率。</p>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16882115999731688211599429.png" alt="16882115999731688211599429.png"></p>
<p>大数定理将属于数理统计的平均值和属于概率论的期望联系在了一起。</p>
<p><font color='red'>中心极限定理</font><br>
大量（n→∞）、独立、同分布的随机变量之和，近似服从于一维正态分布。</p>
<p>n 个独立同分布的随机变量，当 n 充分大时，其均值服从正态分布。（大量独立同分布的随机变量之和近似服从一维正态分布。）</p>
<p>中心极限定理是说当样本数量无穷大的时候，样本均值的分布呈现正态分布。</p>
<p>实验次数越多，样本均值的分布越趋向于正态分布。</p>
<p>中心极限定理指的是给定一个任意分布的总体。每次从这些总体中随机抽取 n 个抽样，一共抽 m 次。 然后把这 m 组抽样分别求出平均值。这些平均值的分布接近正态分布。</p>
<p>最大似然估计（极大似然估计）是什么？<br>
极大似然估计就是一种参数估计方法。</p>
<p>最大似然估计的目的是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</p>
<p>原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p>
<p>方程的解只是一个估计值，只有在样本数趋于无限多的时候，它才会接近于真实值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[1]   写出似然函数；</span><br><span class="line">[2]   对似然函数取对数，并整理；</span><br><span class="line">[3]   求导数；</span><br><span class="line">[4]   解似然方程。</span><br><span class="line"></span><br><span class="line">[1]   比其他估计方法更加简单；</span><br><span class="line">[2]   收敛性：无偏或者渐近无偏，当样本数目增加时，收敛性质会更好；</span><br><span class="line">[3]   如果假设的类条件概率模型正确，则通常能获得较好的结果。但如果假设模型出现偏差，将导致非常差的估计结果。</span><br></pre></td></tr></table></figure>
<p><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16882131129631688213111956.png" alt="16882131129631688213111956.png"></p>
<p>什么是集合？<br>
由离散个体构成的整体的称为集合，称这些个体为集合的元素。</p>
<p>集合元素的性质：无序性、相异性、确定性、任意性</p>
<ul>
<li><strong>等价关系</strong>：设 R 为非空集合 A 上的一个关系，如果 R 是<strong>自反的</strong>、<strong>对称的</strong>和<strong>传递的</strong>，则称 R 为 A 上的<strong>等价关系</strong>。</li>
<li><strong>等价类</strong>：设 R 是集合 A 上的<strong>等价关系</strong>，与 A 中的一个元素 a <strong>有关系</strong>的所有元素的集合叫做 a 的<strong>等价类</strong>。</li>
</ul>
<p>什么是哈密顿图？</p>
<p>能走出一条通过每个结点仅一次的回路。</p>
<blockquote>
<p>平凡图是哈密顿图（平凡图：仅有一个结点的图）。</p>
</blockquote>
<p>什么是欧拉图？</p>
<p>能走出一条通过每条边仅一次的回路。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/Leevan001">如风</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.liyifan001.top/2023/10/09/%E5%A4%8F%E4%BB%A4%E8%90%A5%E9%9A%8F%E8%AE%B0/">https://blog.liyifan001.top/2023/10/09/%E5%A4%8F%E4%BB%A4%E8%90%A5%E9%9A%8F%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.liyifan001.top" target="_blank">早早起床，拥抱太阳</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%8E%A8%E5%85%8D%E5%A4%8F%E4%BB%A4%E8%90%A5/">推免夏令营</a></div><div class="post_share"><div class="social-share" data-image="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/166832600130917317FFACC42E4B5729AE3767B0311D0.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/10/09/%E4%BF%9D%E7%A0%94%E5%BF%83%E8%B7%AF%E5%8E%86%E7%A8%8B/" title="保研心路历程"><img class="cover" src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16683257273091668325726539.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-09</div><div class="title">保研心路历程</div></div></a></div><div><a href="/2023/10/09/%E5%93%88%E5%B7%A5%E6%B7%B1cs%E9%A2%84%E6%8E%A8%E5%85%8D/" title="哈工深cs预推免"><img class="cover" src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16682742001221499171933.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-09</div><div class="title">哈工深cs预推免</div></div></a></div><div><a href="/2023/10/09/%E8%8B%B1%E8%AF%AD%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/" title="英语面试准备"><img class="cover" src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/1668325987314FA3E3F866C40A665F46EF56BBD227395.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-09</div><div class="title">英语面试准备</div></div></a></div><div><a href="/2023/10/09/%E9%9D%A2%E8%AF%95/" title="面试"><img class="cover" src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/166832600130917317FFACC42E4B5729AE3767B0311D0.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-09</div><div class="title">面试</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/1668273103133D8802D069515E0A42BC91FDE752B8A5C.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">如风</div><div class="author-info__description">热爱睡觉的小笨蛋一枚吖~</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">34</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Leevan001"><i></i><span>🛴前往小家...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Leevan001" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liyifancqu@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">AIlab面试题目准备</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.1.</span> <span class="toc-text">如何防止过拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gan%E7%9A%84%E6%80%9D%E6%83%B3"><span class="toc-number">1.2.</span> <span class="toc-text">GAN的思想</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC"><span class="toc-number">1.2.1.</span> <span class="toc-text">对抗样本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#batch-normalization%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-%E5%AE%83%E5%9C%A8%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E6%98%AF%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB"><span class="toc-number">1.3.</span> <span class="toc-text">Batch normalization介绍一下? 它在训练和测试是有什么区别?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vision-transformer"><span class="toc-number">1.4.</span> <span class="toc-text">Vision Transformer?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E8%BE%93%E5%85%A5%E6%A3%80%E6%B5%8B%E5%99%A8%E4%B9%8B%E5%89%8D%E8%A6%81%E5%81%9A%E4%BB%80%E4%B9%88%E6%93%8D%E4%BD%9C-attention%E7%9A%84%E4%BD%9C%E7%94%A8%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.5.</span> <span class="toc-text">目标检测中输入检测器之前要做什么操作? Attention的作用是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lstm"><span class="toc-number">1.6.</span> <span class="toc-text">LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer"><span class="toc-number">1.7.</span> <span class="toc-text">Transformer</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">数据结构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">计算机网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">操作系统</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">AI</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">数学</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/09/%E4%BF%9D%E7%A0%94%E5%BF%83%E8%B7%AF%E5%8E%86%E7%A8%8B/" title="保研心路历程"><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16683257273091668325726539.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="保研心路历程"/></a><div class="content"><a class="title" href="/2023/10/09/%E4%BF%9D%E7%A0%94%E5%BF%83%E8%B7%AF%E5%8E%86%E7%A8%8B/" title="保研心路历程">保研心路历程</a><time datetime="2023-10-09T12:04:46.000Z" title="发表于 2023-10-09 20:04:46">2023-10-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/09/%E5%93%88%E5%B7%A5%E6%B7%B1cs%E9%A2%84%E6%8E%A8%E5%85%8D/" title="哈工深cs预推免"><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16682742001221499171933.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="哈工深cs预推免"/></a><div class="content"><a class="title" href="/2023/10/09/%E5%93%88%E5%B7%A5%E6%B7%B1cs%E9%A2%84%E6%8E%A8%E5%85%8D/" title="哈工深cs预推免">哈工深cs预推免</a><time datetime="2023-10-09T11:46:52.000Z" title="发表于 2023-10-09 19:46:52">2023-10-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/09/%E5%A4%A7%E9%BA%A6%E7%BD%91%E7%BB%B4%E6%9D%83/" title="大麦网维权"><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/166832600130917317FFACC42E4B5729AE3767B0311D0.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大麦网维权"/></a><div class="content"><a class="title" href="/2023/10/09/%E5%A4%A7%E9%BA%A6%E7%BD%91%E7%BB%B4%E6%9D%83/" title="大麦网维权">大麦网维权</a><time datetime="2023-10-09T11:44:57.000Z" title="发表于 2023-10-09 19:44:57">2023-10-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/09/%E8%8B%B1%E8%AF%AD%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/" title="英语面试准备"><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/1668325987314FA3E3F866C40A665F46EF56BBD227395.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="英语面试准备"/></a><div class="content"><a class="title" href="/2023/10/09/%E8%8B%B1%E8%AF%AD%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/" title="英语面试准备">英语面试准备</a><time datetime="2023-10-09T11:43:40.000Z" title="发表于 2023-10-09 19:43:40">2023-10-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/09/%E9%9D%A2%E8%AF%95/" title="面试"><img src="https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/166832600130917317FFACC42E4B5729AE3767B0311D0.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="面试"/></a><div class="content"><a class="title" href="/2023/10/09/%E9%9D%A2%E8%AF%95/" title="面试">面试</a><time datetime="2023-10-09T11:41:48.000Z" title="发表于 2023-10-09 19:41:48">2023-10-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://fastly.jsdelivr.net/gh/Leevan001/pictureBed@main/utools/16683306913121668330690512.png')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By 如风</div><div class="footer_custom_text">广阔天地，大有作为</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>